{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BDxNhJCKi1-q"
   },
   "source": [
    "# <font color = 'orange'>**Multilayer Perceptron (MLP)**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lfH_fQQNtDJe"
   },
   "source": [
    "A multilayer perceptron is a basic neural neural network where the hidden layers are densely connected vectors i.e., every neuron in one layer is connected to every other neuron in the next layer.<br/>\r\n",
    "In this tutorial we are going to construct one such network to classify the MNIST handwritten digit database.\r\n",
    "We will build the network progressively, evolving it at every step, to fit our needs.<br/>\r\n",
    "The purpose of this tutorial is to convey the process of gaining an intuition about the architecture of a neural network that suits a particular type of deep learning problem.<br/><br/>\r\n",
    "\r\n",
    "**Please Note:** Due to the stochastic nature of our model, your results might differ from previous ones already documented. Please take the accompanying text with a grain of salt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wpmy3MUQvtug"
   },
   "source": [
    "---------------------\r\n",
    "\r\n",
    "Keras has an API to directly load the MNIST digit dataset neatly split into training and testing sets along with relevant labels.\r\n",
    "<font size='5'>``tensorflow.keras.datasets.mnist``<font/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0PzaKK2TUpY1"
   },
   "outputs": [],
   "source": [
    "import numpy as np\r\n",
    "from keras.initializers import RandomNormal\r\n",
    "from keras.layers import Activation, Dense, Dropout, Flatten, ReLU\r\n",
    "from keras.models import Model\r\n",
    "from keras.models import Input\r\n",
    "from keras.optimizers import Adam\r\n",
    "from keras.utils import to_categorical, plot_model\r\n",
    "from tensorflow.keras.datasets import mnist\r\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Md6nYmMF4_1_"
   },
   "source": [
    "---------------\r\n",
    "\r\n",
    "## <font color='orange'>**Dataset**<font/>\r\n",
    "\r\n",
    "MNIST (Modified National Institute of Standards and Technology) digits is a collection of handwritten digits ranging from 0 to 9. It has a training set of 60,000 images, and 10,000 test images that are classified into corresponding categories or labels.<br/>\r\n",
    "They are grayscale images (single channel specifying pixel intensity) of 28 x 28 pixels.<br/><br/>\r\n",
    "The ``mnist.load_data()`` function sets up the training and testing sets, along with the assigned labels for you, as shown below. You can also download the dataset separately from here [The MNIST Database](http://yann.lecun.com/exdb/mnist/) and set it up to be loaded from your Google Drive.<br/><br/>\r\n",
    "This is, kind of the entryway, to test your image based models. The images here are simple (in terms of features to be learnt), in ample number, and are **\"real world\"** samples.<br/><br/>\r\n",
    "\"*If it doesn't work on MNIST, it won't work on anything else.<br/>*\r\n",
    "*If it works on MNIST, it might not work on everything else.\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p2CSSXYxWn-I",
    "outputId": "3b3f9488-524a-42ec-a6a3-bccadc000195"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zcWQk-_YXgYA",
    "outputId": "5355a16e-25e1-4b1c-e25a-fe703bee1540"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "# Shows the number of images and their dimensions respectively\r\n",
    "print(x_train.shape, x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 157
    },
    "id": "mQO_9hmDXuqb",
    "outputId": "754341e5-76f8-432c-a2e0-bfa527c49b54"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f1941b60828>"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHsAAAB7CAYAAABUx/9/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAEc0lEQVR4nO2dSyhtURzG7/EmEmKsFBMDeQzkMZAkI2JCxooRyYRSKEkpxEQdj6G8khnKRJGJGEhGRCjlUeSRx53t7rfuvWfjPOxzvu83Wl+rc/bq/lrnf9ex1lmuj4+PX4KDsJ8egAgckk2EZBMh2URINhGSTUSETb/WZcGH638dmtlESDYRkk2EZBMh2URINhGSTYRkEyHZREg2EZJNhGQTIdlESDYRkk2EZBMh2URINhF225KClvf3d8jPz8+ffu3MzAzkh4cHyAcHB5CHh4chd3Z2Qh4bG4McGxsLeWhoyGo3Nzd/epxfRTObCMkmQrKJcGzNvru7g/z29gZ5b28P8urqKuTb21vIExMTPhtbeno65Pb2dshutxtyYmIi5JKSEshlZWU+G5snNLOJkGwiJJsIl80vLwTs+M/Z2RnknJwcyDc3N4Eayl+EheGcWFtbg2yum03S0tIgx8fHQ05NTfVidH+h4z9CsqmQbCIcU7MfHx8h5+fnQz48PPTZsyoqKiCnpKRAXlxchBwdHQ35J///8AlUs4VkUyHZRDjmu3FzrTo9PQ15fn4ecmFhIeTa2lqP719cXGy1l5eXoS8qKgry5eUl5JGREY/vHSxoZhMh2URINhGOWWfbYe4hM+usue9rcHAQ8sbGhtUuLS318egchdbZQrKpkGwiHLPOtsP8ftokKSnJY//o6KjVNveAuVz/LXMhhWY2EZJNhGQTETTrbDteXl4gNzQ0QF5aWrLa5p7z7Oxs/w0s8GidLSSbCskmImRqtsn19TXkjIwMq52cnAx91dXVkIuKiiDX1NRAdvi6XDVbSDYVIfsxbrKzs2O1Kysroc88HmwyOTkJ2dwCZR7n+WH0MS4kmwrJJoKmZv/JxcUF5La2Nshzc3MeX9/V1QW5o6MDckJCghej8xrVbCHZVEg2EZQ12+Tp6Qny9vY25PLycsjmv1ldXR3k2dlZH47uy6hmC8mmQrKJUM3+BOY25tfXV8gREbgje39/H3JWVpZ/BvZvVLOFZFMh2UQEzfEfX3J+fg7Z/Cmsra0tyGaNNikoKICcmZnpxej8h2Y2EZJNhGQTEbI1++rqCvL4+LjVnpqagj7z56/tCA8Ph2xeI+HUrcaa2URINhGSTUTQ1uz7+3vIKysrkHt7eyEfHR19+1nmVUwDAwOQ8/Lyvv3egUQzmwjJJkKyiXBszTavMT49PYXc2NgIeXd399vPMq+R6OnpgWx+9+3UdbQdmtlESDYRkk3Ej9Vs82qn1tZWyJubm5C9veqpqqrKand3d0OfeRVkZGSkV89yKprZREg2EZJNhF9r9vHxsdXu7++HvvX1dcgnJydePSsuLg5yX18f5JaWFqttXjnBgmY2EZJNhF8/xhcWFqy22+3+0mtzc3Mh19fXQzaP3DQ1NUGOiYn50vMY0MwmQrKJkGwidGQ39NCRXSHZVEg2EZJNhGQTIdlESDYRkk2EZBMh2URINhF2f88OznMu4p9oZhMh2URINhGSTYRkEyHZRPwGuqsCN7mxtxcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(2,2))   # 2 X 2 inches, 1 inch ~ 80 pixels\r\n",
    "plt.axis(\"off\")\r\n",
    "plt.imshow(x_train[0], cmap='Greys')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y_yn9GUH-OsO"
   },
   "source": [
    "------------\r\n",
    "The code below is to extract only a specific class of digits from the dataset, should you wish to work on a smaller scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DezLuF9acehR"
   },
   "outputs": [],
   "source": [
    "# collect specific digits\r\n",
    "\r\n",
    "# train_filter = np.where((Y_train == 0 ) | (Y_train == 4))\r\n",
    "# test_filter = np.where((Y_test == 0) | (Y_test == 4))\r\n",
    "\r\n",
    "# X_train, Y_train = X_train[train_filter], Y_train[train_filter]\r\n",
    "# X_test, Y_test = X_test[test_filter], Y_test[test_filter]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bV4ttPmH-pkz"
   },
   "source": [
    "-----------\r\n",
    "\r\n",
    "## <font color='orange'>**Dataset Distribution**<font/>\r\n",
    "\r\n",
    "Let us now see how the data is distributed among all the classes.<br/><br/>\r\n",
    "It is an important thing to check if the dataset is distributed evenly (with some margin allowed) among all classes, otherwise our model might be trained with a bias against a certain set of classes, and this might might work against it during testing or application.<br/><br/>\r\n",
    "From the plotted histogram we see that the data is quite evenly distributed, so we can proceed further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 388
    },
    "id": "EH0WlSyWeD_y",
    "outputId": "a43084bb-9a94-4fbf-82d4-41fbabd9716d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([5923., 6742., 5958., 6131., 5842., 5421., 5918., 6265., 5851.,\n",
       "        5949.]),\n",
       " array([0. , 0.9, 1.8, 2.7, 3.6, 4.5, 5.4, 6.3, 7.2, 8.1, 9. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# checking for even distribution\r\n",
    "\r\n",
    "plt.figure(figsize=(10,5))\r\n",
    "plt.hist(y_train, edgecolor='white', linewidth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iLFwhmd6DTM0"
   },
   "source": [
    "---------------\r\n",
    "\r\n",
    "## <font color='orange'>**Categorical Labels vs. One-Hot Encoding**<font/>\r\n",
    "\r\n",
    "The labels provided to us via the MNIST represent their respective digits i.e., a handwritten digit of 7 is labelled as '7' (categorical data). While this is conveninet for us, our nueral network will not see it that way.<br/><br/>\r\n",
    "\r\n",
    "Although our dataset is that of digits, there really isn't any ordinal relationship among them, nor do we want there to be. The expectancy of our model should not be to classify the next digit image as + 1 of the previous image.<br/>\r\n",
    "\r\n",
    "Using the provided labels will allow the model to assume a natural ordering between categories and may result in poor performance or unexpected results (predictions halfway between categories, or it assumes higher the categorical value, better the category.).<br/><br/>\r\n",
    "\r\n",
    "To mitigate this problem, we use one-hot encoding (as shown below), where '0' indicates non-existant and '1' indicates existant.<br/><br/>\r\n",
    "\r\n",
    "The list indices where '1' is encoded represent the respective digit i.e., for digit '5', the fifth index will be '1' and the rest will be '0'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jc1YQC5LjRZl",
    "outputId": "3fc2cfe0-8600-4fa5-e66f-2d4bb88613be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 0 4 1 9 2 1 3 1 4]\n"
     ]
    }
   ],
   "source": [
    "# Raw labels\r\n",
    "\r\n",
    "print(y_train[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PflIjvnffts9",
    "outputId": "4fb6e588-d53f-42db-f79b-57d7bd4ba2ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# One-hot encoded labels (compare above)\r\n",
    "\r\n",
    "y_train_cat = to_categorical(y_train)\r\n",
    "y_test_cat = to_categorical(y_test)\r\n",
    "\r\n",
    "print(y_train_cat[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XhHjqA-AJvl9"
   },
   "source": [
    "-----------------\r\n",
    "\r\n",
    "## <font color='orange'>**Dataset Normalization**<font/>\r\n",
    "\r\n",
    "Neural networks tend to work better with small values. A good way to judge this, is to check the range of your activation function in the last layer, or your output layer.<br/>\r\n",
    "Large values might grow out of bounds and corrupt the network through subsquent training steps.<br/><br/>\r\n",
    "\r\n",
    "Therefore, we normalize our image pixels values from [ 0, 255 ] to [ 0, 1 ]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jnR0WlEBkYaJ"
   },
   "outputs": [],
   "source": [
    "# normalization\r\n",
    "x_train_reg = x_train.astype('float32') / 255.0\r\n",
    "x_test_reg = x_test.astype('float32') / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ul2T89SXLSfe"
   },
   "source": [
    "--------------\r\n",
    "\r\n",
    "## <font color='orange'>**Classifier Architecture**<font/>\r\n",
    "\r\n",
    "We will first start with a relatively simple neural network. As mentioned, the network will comprise of fully connected, 1-D hidden layers. The ``Dense()`` layer provided by Keras will be useful here. Documentation for the same can be found [here](https://keras.io/api/layers/core_layers/dense/).<br/><br/>\r\n",
    "\r\n",
    "We are now required to pass a 28 x 28 \"square\" data structure (MNIST image) to a vector (hidden layer). These are actually all tensors. More on that here: [ [1](https://www.tensorflow.org/guide/tensor) ], [ [2](https://en.wikipedia.org/wiki/Tensor) ].<br/><br/>\r\n",
    "\r\n",
    "This incompatibility can be resolved by flattening the square image into a vector (1-D array), by using the ``Flatten()`` layer in Keras ([documentation](https://keras.io/api/layers/reshaping_layers/flatten/)). This layer simply multiplies the dimensions of an n-dimensional object to get the number of units in the vector. So here, 28*28 will give us 784.<br/><br/>\r\n",
    "\r\n",
    "We are also initializing the weights of the neural network ``init = RandomNormal(stddev=0.02)`` to values randomly sampled from a standard normal distribution. This helps with convergence and performance of the network.<br/><br/>\r\n",
    "\r\n",
    "The output layer will obviously contain 10 units sice we want our classifier to output probabilities for 10 classes of digits.<br/><br/>\r\n",
    "\r\n",
    "A neural network has the tendency to memorize its training data, especially if it contains more than enough capacity. In such cases, the network fails catastrophically when subjected to the test data. This is the classic case of the network failing to generalize. To avoid this tendency, the model uses a regularizing layer or function. A common regularizing layer is ``Dropout()``.<br/><br/>\r\n",
    "\r\n",
    "The idea of dropout is simple. Given a dropout rate (passed as an argument), the Dropout layer randomly removes that fraction of units from participating in the next layer. For example, if the first layer has 256 units, after dropout = 0.45 is applied, only (1 - 0.45) * 256 units = 140 units from layer 1 participate in layer 2.<br/><br/>\r\n",
    "\r\n",
    "The Dropout layer makes neural networks robust to unforeseen input data because\r\n",
    "the network is trained to predict correctly, even if some units are missing. It's worth\r\n",
    "noting that dropout is not used in the output layer and it is only active during\r\n",
    "training. Moreover, dropout is not present during predictions.<br/><br/>\r\n",
    "\r\n",
    "The optimizer \"sgd\" used here is Stochastic Gradient Descent. In SGD, a mini batch of samples is chosen to compute an approximate value of the descent.<br/>\r\n",
    "\r\n",
    "**NOTE:** Since optimization is based on differentiation, it follows that an important criterion of the loss function must be for it to be smooth or differentiable. This is an important constraint to keep in mind when introducing a new loss function.<br/><br/>\r\n",
    "\r\n",
    "How far the predicted tensor is from the one-hot ground truth vector is called loss. One type of loss function is mean_squared_error (MSE), or the average of the squares of the differences between the target or label and the prediction .<br/>\r\n",
    " In the current example, we are using **categorical_crossentropy**. It's the negative of the sum of the product of the target or label and the logarithm of the prediction per category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MfIaujCnmblM"
   },
   "outputs": [],
   "source": [
    "def define_classifier(image_shape):\r\n",
    "\r\n",
    "  init = RandomNormal(stddev=0.02)\r\n",
    "  in_image = Input(shape=image_shape)\r\n",
    "\r\n",
    "  c = Flatten()(in_image)\r\n",
    "\r\n",
    "  c = Dense(16, activation='relu', kernel_initializer=init)(c)\r\n",
    "  c = Dropout(0.1)(c)\r\n",
    "\r\n",
    "  c = Dense(32, activation='relu', kernel_initializer=init)(c)\r\n",
    "  c = Dropout(0.1)(c)\r\n",
    "\r\n",
    "  c = Dense(64, activation='relu', kernel_initializer=init)(c)\r\n",
    "  c = Dropout(0.1)(c)\r\n",
    "\r\n",
    "  out = Dense(10, activation = 'softmax', kernel_initializer=init)(c)\r\n",
    "\r\n",
    "  model = Model(in_image, out)\r\n",
    "  model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\r\n",
    "    \r\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gVhbFBq5YPHL"
   },
   "source": [
    "--------------\r\n",
    "\r\n",
    "We finally create the model object by calling the ``define_classifier()`` function, and print out its structure.<br/><br/>\r\n",
    "\r\n",
    "Please note the **output shape** of each layer and the **number of parameters**.\r\n",
    "\r\n",
    "The \"None\" shown as the first dimension of each output shape tell us that the network is maleable to that dimension and we can pass any value in it. For example in ``[(None, 28, 28, 1)]`` the ``(28, 28, 1)`` represents the image shape and ``None`` here means we can pass any number images at a time. But they have to be reshaped properly (in a tensor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jNOxSP1Vr_bF",
    "outputId": "e067ac77-62e6-4a61-eb72-c618b5d83f45"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                12560     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 15,866\n",
      "Trainable params: 15,866\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = define_classifier((28,28,1))\r\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ratbTP_paXvc"
   },
   "source": [
    "---------------\r\n",
    "\r\n",
    "## <font color='orange'>**Training the model**<font/>\r\n",
    "\r\n",
    "We use the model object to call ``fit()`` and pass our normalized training images ``x_train_reg`` and corresponding one-hot encoded labels ``y_train_cat`` as arguments.\r\n",
    "\r\n",
    "We train it for 10 **epochs** for now. One Epoch is when an **entire** dataset is passed forward and backward through the neural network only **once**.<br/>\r\n",
    "Since one epoch is too big to feed to the computer at once we divide it in several smaller batches.<br/><br/>\r\n",
    "\r\n",
    "Hence, we pass our dataset in batches of 64, shown by ``batch_size``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kn8XkyDRsOUG",
    "outputId": "4ee996e7-a073-49c2-e6d6-0d75eae81976"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "938/938 [==============================] - 2s 1ms/step - loss: 2.3021 - accuracy: 0.1130\n",
      "Epoch 2/10\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 2.3013 - accuracy: 0.1122\n",
      "Epoch 3/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 2.3013 - accuracy: 0.1103\n",
      "Epoch 4/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 2.3012 - accuracy: 0.1108\n",
      "Epoch 5/10\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 2.3011 - accuracy: 0.1123\n",
      "Epoch 6/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 2.3005 - accuracy: 0.1152\n",
      "Epoch 7/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 2.3014 - accuracy: 0.1110\n",
      "Epoch 8/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 2.3009 - accuracy: 0.1126\n",
      "Epoch 9/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 2.3011 - accuracy: 0.1112\n",
      "Epoch 10/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 2.3007 - accuracy: 0.1116\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f1902e51208>"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train_reg, y_train_cat, epochs=10, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tYTLMEqCcPJR"
   },
   "source": [
    "----------------\r\n",
    "\r\n",
    "## <font color='orange'>**Welcome to Practical Deep Learning. It's hell, and you're gonna love it.**<font/>\r\n",
    "\r\n",
    "Checking the **training** accuracy above, we see that it is **11%**. :(<br/><br>\r\n",
    "\r\n",
    "Perhaps our network is not \"deep\" enough i.e., there aren't enough parameters to tune such that our process of classification might be encoded. Let us increase its \"depth\".<br/><br/>\r\n",
    "\r\n",
    "Since, our training accuracy is so bad, we won't be testing/evaluating it just yet. Still, for a perspective, the code below calls on the model to predict one single image.<br/>\r\n",
    "\r\n",
    "As you see, the image is that of digit '9' but it is classified as '5'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zN8nTnAAuvFi"
   },
   "outputs": [],
   "source": [
    "# input1 = np.reshape(x_test_reg[7],(1,28,28,1))\r\n",
    "# out = model.predict(input1)\r\n",
    "# plt.imshow(x_test_reg[7], cmap='Greys')\r\n",
    "# print(np.argmin(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bI2KILTeggjG"
   },
   "source": [
    "----------\r\n",
    "\r\n",
    "## <font color='orange'>**Mk. 2**<font/>\r\n",
    "\r\n",
    "We have added two additional layers of 128 and 256 units each, and increased the corresponding dropout rate, given the increased number of units in the layer.<br/><br/>\r\n",
    "\r\n",
    "Let us see how this changes things.\r\n",
    "\r\n",
    "Note the increased number of parameters in the model summary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aRvDgxaffVjA"
   },
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ivUCW0JMJal4"
   },
   "outputs": [],
   "source": [
    "def define_classifier(image_shape):\r\n",
    "\r\n",
    "  init = RandomNormal(stddev=0.02)\r\n",
    "  in_image = Input(shape=image_shape)\r\n",
    "\r\n",
    "  c = Flatten()(in_image)\r\n",
    "\r\n",
    "  c = Dense(32, activation='relu', kernel_initializer=init)(c)\r\n",
    "  c = Dropout(0.1)(c)\r\n",
    "\r\n",
    "  c = Dense(64, activation='relu', kernel_initializer=init)(c)\r\n",
    "  c = Dropout(0.1)(c)\r\n",
    "\r\n",
    "  c = Dense(128, activation='relu', kernel_initializer=init)(c)\r\n",
    "  c = Dropout(0.2)(c)\r\n",
    "\r\n",
    "  c = Dense(256, activation='relu', kernel_initializer=init)(c)\r\n",
    "  c = Dropout(0.3)(c)\r\n",
    "\r\n",
    "  out = Dense(10, activation = 'softmax', kernel_initializer=init)(c)\r\n",
    "\r\n",
    "  model = Model(in_image, out)\r\n",
    "  model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\r\n",
    "    \r\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_UFdYCSGKEwx",
    "outputId": "bc26ad8f-2949-4abb-bda3-9699eebfd90f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 71,146\n",
      "Trainable params: 71,146\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = define_classifier((28,28,1))\r\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zJ8acaqgKFsn",
    "outputId": "cf5b070d-f7d0-41c4-e437-7a5a9fbc2f2c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 2.3021 - accuracy: 0.1129\n",
      "Epoch 2/10\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 2.3013 - accuracy: 0.1115\n",
      "Epoch 3/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3013 - accuracy: 0.1110\n",
      "Epoch 4/10\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 2.3012 - accuracy: 0.1126\n",
      "Epoch 5/10\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 2.3010 - accuracy: 0.1109\n",
      "Epoch 6/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3011 - accuracy: 0.1128\n",
      "Epoch 7/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1127\n",
      "Epoch 8/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3016 - accuracy: 0.1101\n",
      "Epoch 9/10\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 2.3010 - accuracy: 0.1126\n",
      "Epoch 10/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3009 - accuracy: 0.1124\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f190072c630>"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train_reg, y_train_cat, epochs=10, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aZRIHhGphqNT"
   },
   "source": [
    "---------------\r\n",
    "\r\n",
    "## <font color='orange'>**Mk. 3**<font/>\r\n",
    "\r\n",
    "Okay, still bad.<br/><br/>\r\n",
    "\r\n",
    "Maybe the model isn't getting enough chances to properly tune itself according to our needs i.e., maybe the model isn't being trained enough.<br/><br/>\r\n",
    "\r\n",
    "Let us increase the number of epochs to 20 and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rZ_z3zN1KKLv",
    "outputId": "e9f6f9ba-639e-4fdb-9323-5808d0763ebe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 2.3009 - accuracy: 0.1124\n",
      "Epoch 2/20\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 2.3008 - accuracy: 0.1124\n",
      "Epoch 3/20\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 2.3007 - accuracy: 0.1124\n",
      "Epoch 4/20\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3005 - accuracy: 0.1124\n",
      "Epoch 5/20\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 2.3003 - accuracy: 0.1124\n",
      "Epoch 6/20\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2998 - accuracy: 0.1124\n",
      "Epoch 7/20\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2987 - accuracy: 0.1124\n",
      "Epoch 8/20\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 2.2956 - accuracy: 0.1124\n",
      "Epoch 9/20\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 2.2771 - accuracy: 0.1459\n",
      "Epoch 10/20\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0104 - accuracy: 0.2236\n",
      "Epoch 11/20\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.7679 - accuracy: 0.2857\n",
      "Epoch 12/20\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5758 - accuracy: 0.4015\n",
      "Epoch 13/20\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.3501 - accuracy: 0.4759\n",
      "Epoch 14/20\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.2056 - accuracy: 0.5391\n",
      "Epoch 15/20\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.0635 - accuracy: 0.6220\n",
      "Epoch 16/20\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 0.9267 - accuracy: 0.6848\n",
      "Epoch 17/20\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.8097 - accuracy: 0.7326\n",
      "Epoch 18/20\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 0.7149 - accuracy: 0.7703\n",
      "Epoch 19/20\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 0.6329 - accuracy: 0.8037\n",
      "Epoch 20/20\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 0.5707 - accuracy: 0.8274\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f19006460f0>"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# didn't rebuild the model\r\n",
    "\r\n",
    "model.fit(x_train_reg, y_train_cat, epochs=20, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J0qFLtP9ifzb"
   },
   "source": [
    "--------\r\n",
    "\r\n",
    "## <font color='orange'>**Mk. 4**<font/>\r\n",
    "\r\n",
    "Alright! Major improvement. A jump from 11% to **86%** accuracy isn't bad.<br/><br/>\r\n",
    "\r\n",
    "A keen observer might note here that the accuracy starts improving from the 8th epoch, whereas according to our previous run of the model it should at least remain the same till 10th epoch, since we haven't made any changes to the model architecture.<br/><br/>\r\n",
    "\r\n",
    "The reason is that we didn't call ``define_classifier()`` and **rebuild** our model. We called the fit function on the previously trained model itself. This allowed our model to use the knowledge gained from its previous run.<br/>\r\n",
    "This is just something to note, you don't **have** to do this.\r\n",
    "\r\n",
    "-----------\r\n",
    "\r\n",
    "Okay, let's try to improve the accuracy even further. Maybe the depth is still an issue.<br/>\r\n",
    " We add another layer of 256 units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dznq6so9KapK"
   },
   "outputs": [],
   "source": [
    "def define_classifier(image_shape):\r\n",
    "\r\n",
    "  init = RandomNormal(stddev=0.02)\r\n",
    "  in_image = Input(shape=image_shape)\r\n",
    "\r\n",
    "  c = Flatten()(in_image)\r\n",
    "\r\n",
    "  c = Dense(32, activation='relu', kernel_initializer=init)(c)\r\n",
    "  c = Dropout(0.1)(c)\r\n",
    "\r\n",
    "  c = Dense(64, activation='relu', kernel_initializer=init)(c)\r\n",
    "  c = Dropout(0.1)(c)\r\n",
    "\r\n",
    "  c = Dense(128, activation='relu', kernel_initializer=init)(c)\r\n",
    "  c = Dropout(0.2)(c)\r\n",
    "\r\n",
    "  c = Dense(256, activation='relu', kernel_initializer=init)(c)\r\n",
    "  c = Dropout(0.3)(c)\r\n",
    "\r\n",
    "  c = Dense(256, activation='relu', kernel_initializer=init)(c)\r\n",
    "  c = Dropout(0.4)(c)\r\n",
    "\r\n",
    "  out = Dense(10, activation = 'softmax', kernel_initializer=init)(c)\r\n",
    "\r\n",
    "  model = Model(in_image, out)\r\n",
    "  model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\r\n",
    "    \r\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-fd2_CV2K9rO",
    "outputId": "8323aeb6-7cab-4e43-f97e-bbba9c83d88b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 136,938\n",
      "Trainable params: 136,938\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = define_classifier((28,28,1))\r\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H5miYLrrK-cV",
    "outputId": "f615e826-0f87-4a73-cfb3-87f7cc3b743b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "938/938 [==============================] - 4s 4ms/step - loss: 2.3022 - accuracy: 0.1092\n",
      "Epoch 2/20\n",
      "938/938 [==============================] - 4s 4ms/step - loss: 2.3013 - accuracy: 0.1115\n",
      "Epoch 3/20\n",
      "938/938 [==============================] - 4s 4ms/step - loss: 2.3012 - accuracy: 0.1137\n",
      "Epoch 4/20\n",
      "938/938 [==============================] - 4s 4ms/step - loss: 2.3010 - accuracy: 0.1136\n",
      "Epoch 5/20\n",
      "938/938 [==============================] - 4s 5ms/step - loss: 2.3013 - accuracy: 0.1116\n",
      "Epoch 6/20\n",
      "938/938 [==============================] - 4s 5ms/step - loss: 2.3012 - accuracy: 0.1114\n",
      "Epoch 7/20\n",
      "938/938 [==============================] - 4s 4ms/step - loss: 2.3014 - accuracy: 0.1117\n",
      "Epoch 8/20\n",
      "938/938 [==============================] - 4s 4ms/step - loss: 2.3011 - accuracy: 0.1124\n",
      "Epoch 9/20\n",
      "938/938 [==============================] - 4s 4ms/step - loss: 2.3010 - accuracy: 0.1119\n",
      "Epoch 10/20\n",
      "938/938 [==============================] - 4s 4ms/step - loss: 2.3013 - accuracy: 0.1096\n",
      "Epoch 11/20\n",
      "938/938 [==============================] - 4s 4ms/step - loss: 2.3012 - accuracy: 0.1135\n",
      "Epoch 12/20\n",
      "938/938 [==============================] - 4s 4ms/step - loss: 2.3012 - accuracy: 0.1136\n",
      "Epoch 13/20\n",
      "938/938 [==============================] - 4s 4ms/step - loss: 2.3011 - accuracy: 0.1120\n",
      "Epoch 14/20\n",
      "938/938 [==============================] - 4s 4ms/step - loss: 2.3015 - accuracy: 0.1113\n",
      "Epoch 15/20\n",
      "938/938 [==============================] - 4s 4ms/step - loss: 2.3014 - accuracy: 0.1093\n",
      "Epoch 16/20\n",
      "938/938 [==============================] - 4s 4ms/step - loss: 2.3010 - accuracy: 0.1120\n",
      "Epoch 17/20\n",
      "938/938 [==============================] - 4s 4ms/step - loss: 2.3010 - accuracy: 0.1126\n",
      "Epoch 18/20\n",
      "938/938 [==============================] - 4s 4ms/step - loss: 2.3011 - accuracy: 0.1116\n",
      "Epoch 19/20\n",
      "938/938 [==============================] - 4s 4ms/step - loss: 2.3010 - accuracy: 0.1146\n",
      "Epoch 20/20\n",
      "938/938 [==============================] - 4s 4ms/step - loss: 2.3013 - accuracy: 0.1121\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f1900627be0>"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train_reg, y_train_cat, epochs=20, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BnJLrYb_luRC"
   },
   "source": [
    "------------\r\n",
    "\r\n",
    "## <font color='orange'>**Mk. 5**<font/>\r\n",
    "\r\n",
    "Bad choice. Our accuracy has dropped again.<br/><br/>\r\n",
    "\r\n",
    "We can't really say what the reason is here. Because as per our previous experience, increasing the model capacity shouldn't harm the model in this way.<br/>\r\n",
    "\r\n",
    " Maybe we made the model overly complex and it **failed to converge**? We need to investigate this behaviour. Re-build the model and train it multiple times to check if the optimizer isn't the problem<br/><br/>\r\n",
    "\r\n",
    " Let's keep the architecture as it is, and change the optimizer from SGD to Adam (adaptive Moments). More on it [here](https://arxiv.org/abs/1412.6980)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mKr7NhGkLDid"
   },
   "outputs": [],
   "source": [
    "def define_classifier(image_shape):\r\n",
    "\r\n",
    "  init = RandomNormal(stddev=0.02)\r\n",
    "  in_image = Input(shape=image_shape)\r\n",
    "\r\n",
    "  c = Flatten()(in_image)\r\n",
    "\r\n",
    "  c = Dense(32, activation='relu', kernel_initializer=init)(c)\r\n",
    "  c = Dropout(0.1)(c)\r\n",
    "\r\n",
    "  c = Dense(64, activation='relu', kernel_initializer=init)(c)\r\n",
    "  c = Dropout(0.1)(c)\r\n",
    "\r\n",
    "  c = Dense(128, activation='relu', kernel_initializer=init)(c)\r\n",
    "  c = Dropout(0.2)(c)\r\n",
    "\r\n",
    "  c = Dense(256, activation='relu', kernel_initializer=init)(c)\r\n",
    "  c = Dropout(0.3)(c)\r\n",
    "\r\n",
    "  c = Dense(256, activation='relu', kernel_initializer=init)(c)\r\n",
    "  c = Dropout(0.4)(c)\r\n",
    "\r\n",
    "  out = Dense(10, activation = 'softmax', kernel_initializer=init)(c)\r\n",
    "\r\n",
    "  model = Model(in_image, out)\r\n",
    "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
    "    \r\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cusa-53VL-Jb"
   },
   "outputs": [],
   "source": [
    "model = define_classifier((28,28,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rdKYRaqIMCPl",
    "outputId": "16806167-f9a4-43d8-afb7-001e014e5853"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "938/938 [==============================] - 5s 5ms/step - loss: 1.4602 - accuracy: 0.4229\n",
      "Epoch 2/20\n",
      "938/938 [==============================] - 5s 5ms/step - loss: 0.4210 - accuracy: 0.8878\n",
      "Epoch 3/20\n",
      "938/938 [==============================] - 5s 5ms/step - loss: 0.3053 - accuracy: 0.9213\n",
      "Epoch 4/20\n",
      "938/938 [==============================] - 4s 5ms/step - loss: 0.2523 - accuracy: 0.9326\n",
      "Epoch 5/20\n",
      "938/938 [==============================] - 5s 5ms/step - loss: 0.2288 - accuracy: 0.9371\n",
      "Epoch 6/20\n",
      "938/938 [==============================] - 4s 5ms/step - loss: 0.2096 - accuracy: 0.9431\n",
      "Epoch 7/20\n",
      "938/938 [==============================] - 5s 5ms/step - loss: 0.2004 - accuracy: 0.9449\n",
      "Epoch 8/20\n",
      "938/938 [==============================] - 5s 5ms/step - loss: 0.1904 - accuracy: 0.9485\n",
      "Epoch 9/20\n",
      "938/938 [==============================] - 5s 5ms/step - loss: 0.1855 - accuracy: 0.9491\n",
      "Epoch 10/20\n",
      "938/938 [==============================] - 5s 5ms/step - loss: 0.1678 - accuracy: 0.9534\n",
      "Epoch 11/20\n",
      "938/938 [==============================] - 5s 5ms/step - loss: 0.1662 - accuracy: 0.9536\n",
      "Epoch 12/20\n",
      "938/938 [==============================] - 5s 5ms/step - loss: 0.1593 - accuracy: 0.9548\n",
      "Epoch 13/20\n",
      "938/938 [==============================] - 4s 5ms/step - loss: 0.1525 - accuracy: 0.9566\n",
      "Epoch 14/20\n",
      "938/938 [==============================] - 5s 5ms/step - loss: 0.1508 - accuracy: 0.9574\n",
      "Epoch 15/20\n",
      "938/938 [==============================] - 5s 5ms/step - loss: 0.1513 - accuracy: 0.9574\n",
      "Epoch 16/20\n",
      "938/938 [==============================] - 5s 5ms/step - loss: 0.1452 - accuracy: 0.9588\n",
      "Epoch 17/20\n",
      "938/938 [==============================] - 5s 5ms/step - loss: 0.1420 - accuracy: 0.9597\n",
      "Epoch 18/20\n",
      "938/938 [==============================] - 5s 5ms/step - loss: 0.1400 - accuracy: 0.9607\n",
      "Epoch 19/20\n",
      "938/938 [==============================] - 5s 5ms/step - loss: 0.1301 - accuracy: 0.9630\n",
      "Epoch 20/20\n",
      "938/938 [==============================] - 5s 5ms/step - loss: 0.1340 - accuracy: 0.9620\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f18ff46c6d8>"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train_reg, y_train_cat, epochs=20, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gGVuSsRRnON2"
   },
   "source": [
    "--------------\r\n",
    "\r\n",
    "Okay! The accuracy has increased in a major way, and right from the 2nd epoch.<br/>\r\n",
    "You might want to take some time here (not during the lecture :D) to think on why the change to Adam optimizer affected the model this much. Please keep in mind our previous attempts too.<br/><br/>\r\n",
    "\r\n",
    "Alright, with **96%** training accuracy, our model is good enough to test. We need to see if it has **overfitted** on our training data or not.\r\n",
    "\r\n",
    "----------\r\n",
    "\r\n",
    "``model.evaluate`` here does the testing for us. We need to pass our test images ``x_test_reg`` and corresponding labels ``y_test_cat`` as arguments. It will return a list with loss and accuracy values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5KL0p3kCMEh1",
    "outputId": "bf225940-f4cc-4701-f495-910813e4c2c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.13050980865955353\n",
      "Test accuracy: 0.9646999835968018\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test_reg, y_test_cat, verbose=0)\r\n",
    "print(\"Test loss:\", score[0])\r\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UQ7mlthwpN2C"
   },
   "source": [
    "-----------\r\n",
    "\r\n",
    "Testing accuracy is the same as training accuracy, **96%**. Nice!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WdC2c_nepkdR"
   },
   "source": [
    "-------------\r\n",
    "\r\n",
    "## <font color='orange'>**Mk. 6**<font/>\r\n",
    "\r\n",
    "We haven't checked what chaning the ``batch_size`` does to our model. Let's try that and increase the size to 128.<br/>\r\n",
    "The batch size is recommended to be a power of 2 for GPU\r\n",
    "optimization purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CvidNYarMsk9",
    "outputId": "3709841a-c971-446e-9278-2b4412303d33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "469/469 [==============================] - 4s 7ms/step - loss: 1.4961 - accuracy: 0.4284\n",
      "Epoch 2/20\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 0.4284 - accuracy: 0.8757\n",
      "Epoch 3/20\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 0.3168 - accuracy: 0.9124\n",
      "Epoch 4/20\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 0.2683 - accuracy: 0.9248\n",
      "Epoch 5/20\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 0.2413 - accuracy: 0.9318\n",
      "Epoch 6/20\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 0.2289 - accuracy: 0.9365\n",
      "Epoch 7/20\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 0.2057 - accuracy: 0.9421\n",
      "Epoch 8/20\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 0.1991 - accuracy: 0.9447\n",
      "Epoch 9/20\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 0.1951 - accuracy: 0.9470\n",
      "Epoch 10/20\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 0.1740 - accuracy: 0.9513\n",
      "Epoch 11/20\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 0.1704 - accuracy: 0.9531\n",
      "Epoch 12/20\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 0.1642 - accuracy: 0.9552\n",
      "Epoch 13/20\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 0.1529 - accuracy: 0.9570\n",
      "Epoch 14/20\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 0.1519 - accuracy: 0.9574\n",
      "Epoch 15/20\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 0.1490 - accuracy: 0.9586\n",
      "Epoch 16/20\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 0.1436 - accuracy: 0.9603\n",
      "Epoch 17/20\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 0.1450 - accuracy: 0.9596\n",
      "Epoch 18/20\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 0.1326 - accuracy: 0.9633\n",
      "Epoch 19/20\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 0.1330 - accuracy: 0.9629\n",
      "Epoch 20/20\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 0.1434 - accuracy: 0.9602\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f18fe272518>"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = define_classifier((28,28,1))\r\n",
    "model.fit(x_train_reg, y_train_cat, epochs=20, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k0oSfZe9qDMg"
   },
   "source": [
    "------------\r\n",
    "\r\n",
    "## <font color='orange'>**Mk. 7**<font/>\r\n",
    "\r\n",
    "Okay, not that much change. It's actually starting to decrease the accuracy.<br/>\r\n",
    "More on that [here](https://stats.stackexchange.com/questions/164876/what-is-the-trade-off-between-batch-size-and-number-of-iterations-to-train-a-neu).<br/><br/>\r\n",
    "\r\n",
    "Let us increase the depth of our model even further. Since our optimizer choice was so good, we are hoping it can handle a model of increased depth.<br/>\r\n",
    "\r\n",
    "We add another layer of 512 units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wMVdfdyYPS9F"
   },
   "outputs": [],
   "source": [
    "def define_classifier(image_shape):\r\n",
    "\r\n",
    "  init = RandomNormal(stddev=0.02)\r\n",
    "  in_image = Input(shape=image_shape)\r\n",
    "\r\n",
    "  c = Flatten()(in_image)\r\n",
    "\r\n",
    "  c = Dense(64, activation='relu', kernel_initializer=init)(c)\r\n",
    "  c = Dropout(0.1)(c)\r\n",
    "\r\n",
    "  c = Dense(128, activation='relu', kernel_initializer=init)(c)\r\n",
    "  c = Dropout(0.1)(c)\r\n",
    "\r\n",
    "  c = Dense(256, activation='relu', kernel_initializer=init)(c)\r\n",
    "  c = Dropout(0.3)(c)\r\n",
    "\r\n",
    "  c = Dense(256, activation='relu', kernel_initializer=init)(c)\r\n",
    "  c = Dropout(0.3)(c)\r\n",
    "\r\n",
    "  c = Dense(512, activation='relu', kernel_initializer=init)(c)\r\n",
    "  c = Dropout(0.4)(c)\r\n",
    "\r\n",
    "  out = Dense(10, activation = 'softmax', kernel_initializer=init)(c)\r\n",
    "\r\n",
    "  model = Model(in_image, out)\r\n",
    "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
    "    \r\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4zAROySlQzM3",
    "outputId": "9850aafb-82ba-438b-d120-3a5bfec2934e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 64)                50240     \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 294,090\n",
      "Trainable params: 294,090\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = define_classifier((28,28,1))\r\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s1SP-ZNJQ28C",
    "outputId": "a0c4db03-3cb3-4296-cb82-2b81c22853ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "938/938 [==============================] - 8s 8ms/step - loss: 1.0060 - accuracy: 0.6305\n",
      "Epoch 2/20\n",
      "938/938 [==============================] - 8s 8ms/step - loss: 0.2220 - accuracy: 0.9385\n",
      "Epoch 3/20\n",
      "938/938 [==============================] - 7s 8ms/step - loss: 0.1726 - accuracy: 0.9524\n",
      "Epoch 4/20\n",
      "938/938 [==============================] - 7s 8ms/step - loss: 0.1465 - accuracy: 0.9602\n",
      "Epoch 5/20\n",
      "938/938 [==============================] - 7s 8ms/step - loss: 0.1260 - accuracy: 0.9648\n",
      "Epoch 6/20\n",
      "938/938 [==============================] - 8s 8ms/step - loss: 0.1189 - accuracy: 0.9666\n",
      "Epoch 7/20\n",
      "938/938 [==============================] - 8s 8ms/step - loss: 0.1105 - accuracy: 0.9696\n",
      "Epoch 8/20\n",
      "938/938 [==============================] - 8s 8ms/step - loss: 0.0962 - accuracy: 0.9726\n",
      "Epoch 9/20\n",
      "938/938 [==============================] - 8s 8ms/step - loss: 0.0942 - accuracy: 0.9736\n",
      "Epoch 10/20\n",
      "938/938 [==============================] - 8s 8ms/step - loss: 0.0883 - accuracy: 0.9757\n",
      "Epoch 11/20\n",
      "938/938 [==============================] - 7s 8ms/step - loss: 0.0850 - accuracy: 0.9753\n",
      "Epoch 12/20\n",
      "938/938 [==============================] - 8s 8ms/step - loss: 0.0751 - accuracy: 0.9781\n",
      "Epoch 13/20\n",
      "938/938 [==============================] - 8s 9ms/step - loss: 0.0830 - accuracy: 0.9775\n",
      "Epoch 14/20\n",
      "938/938 [==============================] - 8s 8ms/step - loss: 0.0801 - accuracy: 0.9772\n",
      "Epoch 15/20\n",
      "938/938 [==============================] - 8s 9ms/step - loss: 0.0727 - accuracy: 0.9783\n",
      "Epoch 16/20\n",
      "938/938 [==============================] - 11s 12ms/step - loss: 0.0668 - accuracy: 0.9813\n",
      "Epoch 17/20\n",
      "938/938 [==============================] - 8s 9ms/step - loss: 0.0678 - accuracy: 0.9807\n",
      "Epoch 18/20\n",
      "938/938 [==============================] - 7s 8ms/step - loss: 0.0689 - accuracy: 0.9812\n",
      "Epoch 19/20\n",
      "938/938 [==============================] - 8s 8ms/step - loss: 0.0638 - accuracy: 0.9819\n",
      "Epoch 20/20\n",
      "938/938 [==============================] - 8s 8ms/step - loss: 0.0638 - accuracy: 0.9823\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f18fd90b908>"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bigger not always better. > train time. try to increase compactness\r\n",
    "\r\n",
    "model.fit(x_train_reg, y_train_cat, epochs=20, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uYVnfJ7rrA2Y"
   },
   "source": [
    "---------\r\n",
    "\r\n",
    "Okay, we have pushed it beyond **98%**.<br/>\r\n",
    "You might be starting to notice the increased training time now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XZ-sTyiZRGZd",
    "outputId": "356e02ac-18af-485c-b3e2-259bbdd24a62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.09701474756002426\n",
      "Test accuracy: 0.9768999814987183\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test_reg, y_test_cat, verbose=0)\r\n",
    "print(\"Test loss:\", score[0])\r\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n3CQ20dnrVz8"
   },
   "source": [
    "-----------\r\n",
    "\r\n",
    "## <font color='orange'>**Points to Note**<font/>\r\n",
    "\r\n",
    "Test accuracy has decreased a bit but our model is good enough.<br/>\r\n",
    "Please note, that a deeper model might not always be the best idea. It depends on the scenario really.<br/><br/>\r\n",
    "For a nominal percentage increase in accuracy in an image model, building a model that takes twice as long to train is not a good trade-off.<br/>\r\n",
    "This small percentage will have no consequence in production in case of image classification or recognition models.<br/>\r\n",
    "However, a small percentage increase in an ML model used in the financial sector, may equal to millions in revenue.<br/><br/>\r\n",
    "Evaluate your data and scenario and choose your architecture wisely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KKq-Y9Y6s0BL"
   },
   "source": [
    "-------------\r\n",
    "\r\n",
    "## <font color='orange'>**Blind Run**<font/>\r\n",
    "\r\n",
    "Let us now train the same model on a completely new dataset and see how it performs.<br/>\r\n",
    "**We are not going to fine-tune our model here. This is just to see how it holds up against images with increased complexity.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "41s8UFZntdmZ"
   },
   "source": [
    "------------\r\n",
    "\r\n",
    "# <font color='orange'>**Fashion MNIST Dataset**<font/>\r\n",
    "\r\n",
    "This is a dataset of 60,000 28x28 grayscale images of 10 fashion categories, along with a test set of 10,000 images.\r\n",
    "\r\n",
    "(Label, \tDescription)<br/>\r\n",
    "(0, \tT-shirt/top)\r\n",
    "(1, \tTrouser)\r\n",
    "(2, \tPullover)\r\n",
    "(3, \tDress)\r\n",
    "(4, \tCoat)\r\n",
    "(5, \tSandal)\r\n",
    "(6, \tShirt)\r\n",
    "(7, \tSneaker)\r\n",
    "(8, \tBag)\r\n",
    "(9, \tAnkle boot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e9uu5wL3R4WJ"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import fashion_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PURjJ0xKS9Qw",
    "outputId": "16ed97b4-eb7e-47d3-e48f-b1f389be45f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
      "32768/29515 [=================================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "26427392/26421880 [==============================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "8192/5148 [===============================================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "4423680/4422102 [==============================] - 0s 0us/step\n",
      "(60000, 28, 28) (10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\r\n",
    "print(x_train.shape, x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 157
    },
    "id": "pa0dQ-LKTDM4",
    "outputId": "da6873a4-4978-490c-8fee-327522ce52ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHsAAAB7CAYAAABUx/9/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAHaUlEQVR4nO2dSYxNWxSGd+nbiib6oqIv0SUSEU0wQGIipmJgaGZgZiAGIjEyZSaREEMJBkgQA4QBia4Qfd/3PVVvdrP/j3uOqx7Pfev/RufPrjrnVv05d5219tr7NLS3tycTg07/9Qcwfw6bHQibHQibHQibHQibHYguJePOy+qPhmoDvrMDYbMDYbMDYbMDYbMDYbMDYbMDYbMDYbMDYbMDYbMDUVYbr1vYbtXQULVknD59+iS6tbVV9PTp02u6FnWnTr9+T5W1jRX9XcR3diBsdiBsdiBCxuznz5/L2Pbt20X36tWrUHfr1k10c3Nz1Wv9zGcristl8b6tre2nf953diBsdiDCfI3nnDx5UvS+fftEjx49WvTHjx9Fv3v3TvTQoUNFr1ixQnTv3r1F82u+6Gv/8+fPhT/btWvXqr9LfGcHwmYHwmYH4n8bszt37lx17NixY6IvXrwo+suXL6KZ3ixfvlz0iRMnRK9fv1703LlzRU+ZMkV0U1NT5fjy5csydvz4cdHz588XPWHCBNE9evRI1fCdHQibHQibHYiGkim0uln+UzaleeHChcrx6tWrZezx48eiu3fvLroo/qeU0sKFC0VPnDix8Hz8rPfu3ascsxQ7b9480Tt27BC9du1a0S0tLV7+Y2x2KGx2IOomZte6qxNj9pIlSyrHefz+mWux/swYTFgLZ8xn3t3S0lL1Wnv27BF97tw50bdu3eLlHbONzQ6FzQ5E3dTGa2mZ/RGDBg2qHLN+3LdvX9Hv378XzTnl169fi+7Zs6foN2/eiGbM3r9/v+iDBw9Wjr99+yZj9+/fF8258lrwnR0Imx0Imx2IuonZHSXvG2NcpG5sbBSdx/sf6UuXLolmjGbezuvlzwBduqglbA2+fv16+lV8ZwfCZgfCZgeibmJ2rctimRtfvXq1cszlPMy72SfO8T59+oh++vSp6OHDh4tmXv7hwwfR/fv3rxw/e/ZMxjif/eLFC9G3b98WPWrUqFQN39mBsNmBsNmBqJuYzdo4e7nJkSNHROexjTGVa7eYJ7969Up0WUxnbZ3z33yeyK/PfrgNGzaIPn36tGjm7EX4zg6EzQ6EzQ5E3fSg1bKdREop3bx5U/SsWbMqx5x/Ljt32fz1sGHDRHOrLWrOd7PWnsN+ts2bN4tesGABf8U9aMZmh6JDqVdZCTPXHGN6U9Z2VOsugTNnzhSdtx6x3MlUip+FX9Nfv34VzdStrNWYS3zy/wXPzS1B2EJVC76zA2GzA2GzA1FTzC5LUTra7ltEPkWZUkq7d+8WffjwYdFMWfISKWM0t9VgaxDblBhXWR59+/ataP6f+MyQw+lP/uyuXbtEz5gxo+q5iO/sQNjsQNjsQPzWcmkefzhNyKWmDx48EL1z507RnNpjaxGn+pjr5nF03LhxMsZyJmM6PxvzZObZS5curXrtlL5fhpvn2XmLUkrfT4eOHDlS9JkzZxJwudTY7FDY7EDUFLO59GTdunWi7969K/rRo0eVY24fwdx2yJAhohkXGfPLpilZQ542bVrleNu2bTK2aNEi0XzTwMOHD0Uz5yf5thkppfTy5UvR/fr1E53n8Zz+5N/Nc/F5ITlmm5RsdihsdiAKY3ZbW5sMLl68WMavXbsmmjXlPE6zvkwYw2udt33y5IloxtkDBw5Ujvfu3StjGzduFM0lNMzpp06dKnrs2LGir1y5IjrfrjKl75838ryezwusAbAPgB4kx2yTks0Ohc0ORGHMPnTokAyuWrVKxvn2WS4nzTXrzYQ1YMYuxsXx48eL5tJV1srv3LlTOeZrHhgX2YbMVmL2hR09elQ0c34uD+JnK/rf8Fx8tuH2lo2NjY7ZxmaHwmYHorAHjctS+DoEbi/Bfqn8tYWM54xTPBdr5ZMmTRLNmjHzcvag5bX2OXPmyBi3hD5//rxo5vCcKx84cGDhOOsPjOH5M0PZtlp8tmEOX1TP8J0dCJsdCJsdiJpiNvvC+WpA9lrl89uDBw+WMW51wd4q5pOM8cyNeW1uMZXnq3wF8qlTp0Tz2YM9a/x99o3zb+NcftEcAvvGWT9gDGcPGp9tcnxnB8JmB8JmB6IwZo8YMUL0ypUrRW/ZskU069WTJ0+uHDO3ZIxlTGZvFWMZ11txzplxMn/eYC46ZswY0cx1GWOZ6/LZhjUA/u3sDc81e+/42bjdNT0qwnd2IGx2IGx2IDq01uvs2bOiN23aJDqfF2ZfF3unGSc558s4yZjNny96xTLPVbbWi7rsVZEcb25uLvz5/Ppcy33jxg3Rs2fPFr1161aezvPZxmaHovBrvB2DtW6j0draWjles2aNjHHJLtuQ2I7Dr2mWU8umBpuamirHZWVfnovl07KdgHl+poGcfs3/1mXLlskY01mmYj+6fLUB39mBsNmBsNmB+Gt2JWbrD5emsu2IO+5z2pElzgEDBnT0I9YLjtnGZofCZgfir4nZ5l/DMdvY7FDY7EDY7EDY7EDY7EDY7ECUvTbi970HwvxxfGcHwmYHwmYHwmYHwmYHwmYH4h9n5ev+yecD0gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(2,2))   # 2 X 2 inches, 1 inch ~ 80 pixels\r\n",
    "plt.axis(\"off\")\r\n",
    "plt.imshow(x_train[0], cmap='Greys')\r\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WbT_TPgzuZHu"
   },
   "source": [
    "--------------\r\n",
    "\r\n",
    "As you can see from the sample image above, it is noticeably more complex than an image of a handwritten digit.\r\n",
    "\r\n",
    "-----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 387
    },
    "id": "QPnBpSnMTTIg",
    "outputId": "c477560e-f911-433b-8bf5-672c078edcd0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([6000., 6000., 6000., 6000., 6000., 6000., 6000., 6000., 6000.,\n",
       "        6000.]),\n",
       " array([0. , 0.9, 1.8, 2.7, 3.6, 4.5, 5.4, 6.3, 7.2, 8.1, 9. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 33,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check even distribution\r\n",
    "\r\n",
    "plt.figure(figsize=(10,5))\r\n",
    "plt.hist(y_train, edgecolor='white', linewidth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CuIep0DATaRA"
   },
   "outputs": [],
   "source": [
    "# one-hot encoding and normalization\r\n",
    "\r\n",
    "y_train_cat = to_categorical(y_train)\r\n",
    "y_test_cat = to_categorical(y_test)\r\n",
    "x_train_reg = x_train.astype('float32') / 255.0\r\n",
    "x_test_reg = x_test.astype('float32') / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g7ni6KuJTnLT",
    "outputId": "74fd05e5-caad-4140-a0d2-e5f9c86f214e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "938/938 [==============================] - 8s 8ms/step - loss: 1.1698 - accuracy: 0.5196\n",
      "Epoch 2/20\n",
      "938/938 [==============================] - 7s 8ms/step - loss: 0.5601 - accuracy: 0.7916\n",
      "Epoch 3/20\n",
      "938/938 [==============================] - 8s 8ms/step - loss: 0.4760 - accuracy: 0.8272\n",
      "Epoch 4/20\n",
      "938/938 [==============================] - 7s 8ms/step - loss: 0.4323 - accuracy: 0.8467\n",
      "Epoch 5/20\n",
      "938/938 [==============================] - 8s 8ms/step - loss: 0.4007 - accuracy: 0.8569\n",
      "Epoch 6/20\n",
      "938/938 [==============================] - 8s 8ms/step - loss: 0.3849 - accuracy: 0.8658\n",
      "Epoch 7/20\n",
      "938/938 [==============================] - 8s 8ms/step - loss: 0.3648 - accuracy: 0.8691\n",
      "Epoch 8/20\n",
      "938/938 [==============================] - 8s 8ms/step - loss: 0.3557 - accuracy: 0.8757\n",
      "Epoch 9/20\n",
      "938/938 [==============================] - 8s 9ms/step - loss: 0.3438 - accuracy: 0.8772\n",
      "Epoch 10/20\n",
      "938/938 [==============================] - 7s 8ms/step - loss: 0.3392 - accuracy: 0.8773\n",
      "Epoch 11/20\n",
      "938/938 [==============================] - 7s 8ms/step - loss: 0.3350 - accuracy: 0.8812\n",
      "Epoch 12/20\n",
      "938/938 [==============================] - 8s 8ms/step - loss: 0.3330 - accuracy: 0.8807\n",
      "Epoch 13/20\n",
      "938/938 [==============================] - 7s 8ms/step - loss: 0.3219 - accuracy: 0.8854\n",
      "Epoch 14/20\n",
      "938/938 [==============================] - 7s 8ms/step - loss: 0.3162 - accuracy: 0.8899\n",
      "Epoch 15/20\n",
      "938/938 [==============================] - 7s 8ms/step - loss: 0.3129 - accuracy: 0.8873\n",
      "Epoch 16/20\n",
      "938/938 [==============================] - 7s 8ms/step - loss: 0.3139 - accuracy: 0.8880\n",
      "Epoch 17/20\n",
      "938/938 [==============================] - 7s 8ms/step - loss: 0.2999 - accuracy: 0.8927\n",
      "Epoch 18/20\n",
      "938/938 [==============================] - 7s 8ms/step - loss: 0.3002 - accuracy: 0.8927\n",
      "Epoch 19/20\n",
      "938/938 [==============================] - 7s 8ms/step - loss: 0.3055 - accuracy: 0.8907\n",
      "Epoch 20/20\n",
      "938/938 [==============================] - 7s 8ms/step - loss: 0.2951 - accuracy: 0.8934\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f18fc753780>"
      ]
     },
     "execution_count": 35,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = define_classifier((28,28,1))\r\n",
    "model.fit(x_train_reg, y_train_cat, epochs=20, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DaGO4-4HUF4k",
    "outputId": "42e264fa-33c9-4589-ce91-3be831b8d5a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.3620312213897705\n",
      "Test accuracy: 0.8780999779701233\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test_reg, y_test_cat, verbose=0)\r\n",
    "print(\"Test loss:\", score[0])\r\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0UwGFe6Yu7AR"
   },
   "source": [
    "-------------\r\n",
    "\r\n",
    "## <font color='orange'>**Final Notes**<font/>\r\n",
    "\r\n",
    "As your dataset complexity changes, you will need to fine-tune your model accordingly. You might end following our procedure earlier, or might discover a better one.<br/>\r\n",
    "The purpose here was to give an example of how to react to failures when confronting a practical deep learning problem.<br/><br/>\r\n",
    "\r\n",
    "*“Breathe as the tears flow,<br/>\r\n",
    "Breathe when you cannot gaze up high,<br/>\r\n",
    "Breathe as to know the world will lie,<br/>\r\n",
    "Breathe when you choke to ask: “Why?”<br/>\r\n",
    "Breathe even when you are about to die.*\"<br/><br/>\r\n",
    "-Somya Kedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "drcGC7mvUypV"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "MLP.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
